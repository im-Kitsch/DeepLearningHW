{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import collections\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "vocabulary = read_data('text8.zip')\n",
    "print('Data size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "# Filling 4 global variables:\n",
    "# data - list of codes (integers from 0 to vocabulary_size-1).\n",
    "#   This is the original text but words are replaced by their codes\n",
    "# count - map of words(strings) to count of occurrences\n",
    "# dictionary - map of words(strings) to their codes(integers)\n",
    "# reverse_dictionary - maps codes(integers) to words(strings)\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(\n",
    "    vocabulary, vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the']\n",
      "\n",
      "with bag_window = 1:\n",
      "    batch: [['anarchism', 'as'], ['originated', 'a'], ['as', 'term'], ['a', 'of']]\n",
      "    labels: ['originated', 'as', 'a', 'term']\n",
      "\n",
      "with bag_window = 2:\n",
      "    batch: [['anarchism', 'originated', 'a', 'term'], ['originated', 'as', 'term', 'of'], ['as', 'a', 'of', 'abuse'], ['a', 'term', 'abuse', 'first']]\n",
      "    labels: ['as', 'a', 'term', 'of']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "     \n",
    "def generate_batch(batch_size, bag_window):\n",
    "    global data_index\n",
    "    span = 2 * bag_window + 1 # [ bag_window target bag_window ]\n",
    "    batch = np.ndarray(shape=(batch_size, span - 1), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size):\n",
    "        # just for testing\n",
    "        buffer_list = list(buffer)\n",
    "        labels[i, 0] = buffer_list.pop(bag_window) #label hear is [target] shape = (batch_size, 1)\n",
    "        batch[i] = buffer_list #batch hear is [bag_window bag_window]\n",
    "        # iterate to the next buffer\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "     \n",
    "print('data:', [reverse_dictionary[di] for di in data[:16]])\n",
    "     \n",
    "for bag_window in [1, 2]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=4, bag_window=bag_window)\n",
    "    print('\\nwith bag_window = %d:' % (bag_window))\n",
    "    print('    batch:', [[reverse_dictionary[w] for w in bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "bag_window = 2  # How many words to consider left and right.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64  # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zhiyuan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-9-7ca4f271bace>:34: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    " \n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size, bag_window * 2])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    " \n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    " \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embeds = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "        weights = softmax_weights,\n",
    "        biases = softmax_biases, \n",
    "        inputs = tf.reduce_sum(embeds, 1), \n",
    "        labels = train_labels, \n",
    "        num_sampled = num_sampled, \n",
    "        num_classes = vocabulary_size))\n",
    " \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    " \n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    " \n",
    "num_steps = 100001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(128), Dimension(128)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(embeds, 1).get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zhiyuan/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 8.286076\n",
      "Nearest to in: unwritten, vu, ecclesiastica, philistine, sidelined, compassion, longer, soldier,\n",
      "Nearest to known: vos, thenceforth, seizing, before, manama, reformers, treason, utilization,\n",
      "Nearest to to: crispin, lk, allocations, courtship, leftism, commonwealth, megawati, compagnie,\n",
      "Nearest to time: aerodynamic, resurgent, slag, harper, ud, snowmen, workability, bruin,\n",
      "Nearest to th: hardware, drunkenness, conspicuous, shipment, tack, boethius, xxiv, strenuous,\n",
      "Nearest to who: schuschnigg, activex, tabs, martinez, tanzanian, blind, steaming, numerals,\n",
      "Nearest to it: fh, yeo, fundraising, electrician, bamiyan, dahmer, thermoregulation, razzie,\n",
      "Nearest to not: volvo, catholics, cypher, bilge, supervisor, termed, emacs, exmoor,\n",
      "Nearest to years: chitin, climatology, averaging, colitis, nonstop, switches, culture, gal,\n",
      "Nearest to see: simulator, rearguard, bataan, glutamic, communicates, awt, translated, janitor,\n",
      "Nearest to used: monotheists, millennia, kitchen, calvinistic, ghettos, reprints, replacements, spahn,\n",
      "Nearest to five: lascaris, ointment, tz, tarquinius, nfs, ignition, goldie, ns,\n",
      "Nearest to one: shrublands, canaan, disappearing, cognitive, ferrigno, pompeius, irrelevent, enemies,\n",
      "Nearest to may: reassert, gar, kenyan, deva, propped, haihowak, has, implantation,\n",
      "Nearest to eight: locarno, exceeding, infective, overlaid, meighen, syndromes, purview, abrogation,\n",
      "Nearest to been: kale, violated, whipping, voor, musik, kerala, jezebel, nikola,\n",
      "Average loss at step 2000: 4.586309\n",
      "Average loss at step 4000: 3.929104\n",
      "Average loss at step 6000: 3.740798\n",
      "Average loss at step 8000: 3.534514\n",
      "Average loss at step 10000: 3.480361\n",
      "Nearest to in: on, during, mn, of, inr, bannerman, from, avenger,\n",
      "Nearest to known: seen, used, deluded, cluniac, pedantic, fingerprint, regarded, ocaml,\n",
      "Nearest to to: will, would, may, can, must, univ, neoplatonism, lk,\n",
      "Nearest to time: grande, least, point, kipsigis, libels, magnetism, wick, uncontrollably,\n",
      "Nearest to th: selig, hardware, apollo, nine, irr, pds, carte, fuelling,\n",
      "Nearest to who: he, she, accidentally, that, which, regnal, quote, they,\n",
      "Nearest to it: he, she, this, there, assignments, they, mindedness, telugu,\n",
      "Nearest to not: kzin, volvo, cno, aztecs, goat, titans, deutscher, ilinden,\n",
      "Nearest to years: examined, highs, months, pls, year, inventing, days, volution,\n",
      "Nearest to see: bataan, rearguard, trough, aoun, sapporo, pentameter, photographers, persson,\n",
      "Nearest to used: found, known, eugenic, considered, referred, libation, presbyterian, able,\n",
      "Nearest to five: four, seven, zero, three, six, eight, nine, two,\n",
      "Nearest to one: ethnocentric, five, kaist, petrarch, zero, journey, transcriptions, jacobus,\n",
      "Nearest to may: can, would, could, will, should, must, might, cannot,\n",
      "Nearest to eight: six, nine, seven, four, five, three, zero, two,\n",
      "Nearest to been: become, sally, persuasion, jeep, decadence, mastering, laertes, rumours,\n",
      "Average loss at step 12000: 3.497790\n",
      "Average loss at step 14000: 3.439921\n",
      "Average loss at step 16000: 3.437665\n",
      "Average loss at step 18000: 3.405232\n",
      "Average loss at step 20000: 3.217148\n",
      "Nearest to in: during, within, since, on, throughout, urbino, between, kilmer,\n",
      "Nearest to known: regarded, used, seen, considered, referred, well, deluded, rajendra,\n",
      "Nearest to to: will, must, could, would, neoplatonism, may, sideline, should,\n",
      "Nearest to time: point, least, end, recursively, times, beginning, night, tail,\n",
      "Nearest to th: nd, fortran, st, selig, ives, uke, daigo, assaulted,\n",
      "Nearest to who: which, she, he, that, they, analects, sennacherib, and,\n",
      "Nearest to it: he, she, there, mindedness, this, they, masurian, kohanim,\n",
      "Nearest to not: now, aztecs, what, sz, coughing, still, sergei, almost,\n",
      "Nearest to years: months, days, examined, highs, year, decades, creeping, inventing,\n",
      "Nearest to see: pentameter, rearguard, mnr, yangtze, sapporo, triggers, qua, called,\n",
      "Nearest to used: referred, considered, known, found, seen, available, able, presbyterian,\n",
      "Nearest to five: six, seven, three, eight, zero, four, two, nine,\n",
      "Nearest to one: lbf, brisingamen, quits, procedural, oppressive, disappointing, superstar, mourning,\n",
      "Nearest to may: can, would, could, will, must, should, might, cannot,\n",
      "Nearest to eight: nine, seven, six, zero, four, five, three, two,\n",
      "Nearest to been: become, jeep, no, decadence, laertes, typhoon, persuasion, come,\n",
      "Average loss at step 22000: 3.353228\n",
      "Average loss at step 24000: 3.294153\n",
      "Average loss at step 26000: 3.259105\n",
      "Average loss at step 28000: 3.283695\n",
      "Average loss at step 30000: 3.216506\n",
      "Nearest to in: within, during, throughout, on, since, from, of, between,\n",
      "Nearest to known: seen, regarded, referred, used, well, defined, described, possible,\n",
      "Nearest to to: will, would, might, must, neoplatonism, should, shall, could,\n",
      "Nearest to time: least, period, beginning, position, championed, night, point, end,\n",
      "Nearest to th: nd, twentieth, st, rd, ninth, nineteenth, nine, pds,\n",
      "Nearest to who: poaching, spector, sages, young, that, officiated, elie, depictions,\n",
      "Nearest to it: she, he, this, there, prospered, now, what, mindedness,\n",
      "Nearest to not: still, never, now, nothing, coughing, aztecs, killings, sometimes,\n",
      "Nearest to years: months, year, days, decades, hours, centuries, creeping, rounds,\n",
      "Nearest to see: rearguard, nicholls, aoun, triggers, gondor, but, called, whey,\n",
      "Nearest to used: referred, considered, seen, available, found, known, able, designed,\n",
      "Nearest to five: four, six, three, eight, seven, nine, zero, two,\n",
      "Nearest to one: two, eight, tiptree, carducci, greenpeace, crow, rodham, ethnocentric,\n",
      "Nearest to may: would, can, will, could, must, should, might, cannot,\n",
      "Nearest to eight: nine, seven, six, five, four, zero, three, two,\n",
      "Nearest to been: become, come, be, inconsistencies, oasis, westinghouse, jeep, scratching,\n",
      "Average loss at step 32000: 3.006523\n",
      "Average loss at step 34000: 3.204053\n",
      "Average loss at step 36000: 3.193176\n",
      "Average loss at step 38000: 3.156621\n",
      "Average loss at step 40000: 3.164525\n",
      "Nearest to in: within, throughout, during, since, on, through, until, paleo,\n",
      "Nearest to known: regarded, referred, described, seen, defined, used, such, called,\n",
      "Nearest to to: would, can, will, should, must, may, might, parodying,\n",
      "Nearest to time: least, beginning, point, period, end, position, victory, times,\n",
      "Nearest to th: nd, st, twentieth, nineteenth, svalbard, rd, fortran, streaks,\n",
      "Nearest to who: he, whom, franchisee, sages, arrested, licinius, whose, bituminous,\n",
      "Nearest to it: he, she, this, designator, mindedness, there, perpetually, farmed,\n",
      "Nearest to not: still, now, never, also, usually, little, baines, it,\n",
      "Nearest to years: months, days, decades, hours, creeping, year, miles, times,\n",
      "Nearest to see: known, includes, swami, but, incorporates, baskerville, firmus, rearguard,\n",
      "Nearest to used: seen, considered, referred, available, designed, known, produced, applied,\n",
      "Nearest to five: seven, four, six, eight, three, zero, nine, two,\n",
      "Nearest to one: greenpeace, apulia, subtype, lithographs, cuauht, prepares, ethnocentric, overthrew,\n",
      "Nearest to may: can, would, could, should, might, must, will, cannot,\n",
      "Nearest to eight: seven, nine, six, five, four, zero, three, pastures,\n",
      "Nearest to been: become, be, occurred, come, inconsistencies, westinghouse, fallen, kshatriya,\n",
      "Average loss at step 42000: 3.212697\n",
      "Average loss at step 44000: 3.140691\n",
      "Average loss at step 46000: 3.144769\n",
      "Average loss at step 48000: 3.051270\n",
      "Average loss at step 50000: 3.056453\n",
      "Nearest to in: throughout, within, during, paleo, since, until, on, alienate,\n",
      "Nearest to known: used, regarded, referred, seen, defined, described, possible, viewed,\n",
      "Nearest to to: will, can, could, should, cannot, must, towards, would,\n",
      "Nearest to time: period, least, times, point, moment, ingres, length, gojoseon,\n",
      "Nearest to th: nd, st, twentieth, streaks, nineteenth, rd, eicher, third,\n",
      "Nearest to who: which, ecclesiastics, he, young, fermions, living, sennacherib, spector,\n",
      "Nearest to it: she, he, this, there, they, onerous, farmed, artefact,\n",
      "Nearest to not: now, almost, still, nothing, captors, ogilvie, killings, saloons,\n",
      "Nearest to years: decades, months, days, hours, year, creeping, minutes, weeks,\n",
      "Nearest to see: includes, known, called, include, incorporates, rucker, firmus, yangtze,\n",
      "Nearest to used: referred, known, considered, available, seen, applied, designed, found,\n",
      "Nearest to five: four, six, three, eight, seven, two, zero, nine,\n",
      "Nearest to one: altruism, rotates, toolbars, yea, sloan, ien, this, ghazi,\n",
      "Nearest to may: can, could, should, would, must, might, will, cannot,\n",
      "Nearest to eight: nine, seven, five, six, three, four, zero, two,\n",
      "Nearest to been: become, inconsistencies, remained, several, himself, lameness, occurred, interferon,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 52000: 3.089645\n",
      "Average loss at step 54000: 3.087071\n",
      "Average loss at step 56000: 2.921404\n",
      "Average loss at step 58000: 3.019812\n",
      "Average loss at step 60000: 3.058537\n",
      "Nearest to in: within, throughout, during, at, paleo, under, on, since,\n",
      "Nearest to known: regarded, defined, described, used, seen, referred, served, possible,\n",
      "Nearest to to: would, might, will, could, should, must, may, pkn,\n",
      "Nearest to time: period, least, point, end, times, voyage, age, ranges,\n",
      "Nearest to th: nd, twentieth, st, nineteenth, eicher, third, rd, streaks,\n",
      "Nearest to who: she, whom, which, he, playfulness, that, hydroponic, biodiesel,\n",
      "Nearest to it: he, she, this, mindedness, farmed, prospered, fluoxetine, metabolised,\n",
      "Nearest to not: nothing, never, still, now, captors, often, contract, poised,\n",
      "Nearest to years: decades, days, months, hours, minutes, weeks, year, centuries,\n",
      "Nearest to see: includes, but, rucker, regards, include, sextilis, wafers, known,\n",
      "Nearest to used: referred, seen, considered, known, regarded, written, portrayed, available,\n",
      "Nearest to five: six, seven, eight, three, four, nine, zero, two,\n",
      "Nearest to one: two, ien, scrat, bundy, archaeopteryx, clubhouse, four, carducci,\n",
      "Nearest to may: should, can, might, could, would, must, will, cannot,\n",
      "Nearest to eight: seven, nine, five, six, four, three, zero, muscovy,\n",
      "Nearest to been: become, occurred, evolved, emerged, disappeared, come, himself, interferon,\n",
      "Average loss at step 62000: 3.013019\n",
      "Average loss at step 64000: 2.921997\n",
      "Average loss at step 66000: 2.941878\n",
      "Average loss at step 68000: 2.961680\n",
      "Average loss at step 70000: 3.016163\n",
      "Nearest to in: during, throughout, within, since, on, around, into, near,\n",
      "Nearest to known: regarded, described, referred, defined, used, seen, possible, considered,\n",
      "Nearest to to: will, might, must, could, can, cannot, may, should,\n",
      "Nearest to time: period, moment, least, night, gdansk, times, victory, spinoza,\n",
      "Nearest to th: nd, st, twentieth, nine, nineteenth, eicher, rd, third,\n",
      "Nearest to who: young, whom, which, he, that, whose, observables, spector,\n",
      "Nearest to it: he, she, this, farmed, prospered, they, quite, only,\n",
      "Nearest to not: never, almost, now, still, nothing, killings, aztecs, captors,\n",
      "Nearest to years: decades, months, days, year, weeks, minutes, hours, creeping,\n",
      "Nearest to see: includes, known, suborder, saw, aoun, venue, incorporates, hydrocarbons,\n",
      "Nearest to used: referred, seen, introduced, found, considered, described, regarded, designed,\n",
      "Nearest to five: six, seven, four, three, zero, eight, two, athlon,\n",
      "Nearest to one: two, nasl, petrarch, borrow, sloan, kaist, keralite, agassi,\n",
      "Nearest to may: should, must, can, might, could, would, will, cannot,\n",
      "Nearest to eight: nine, six, seven, five, zero, three, two, four,\n",
      "Nearest to been: become, occurred, remained, existed, evolved, disappeared, come, was,\n",
      "Average loss at step 72000: 2.946724\n",
      "Average loss at step 74000: 2.869338\n",
      "Average loss at step 76000: 2.992162\n",
      "Average loss at step 78000: 3.005515\n",
      "Average loss at step 80000: 2.850448\n",
      "Nearest to in: within, throughout, during, on, under, including, until, since,\n",
      "Nearest to known: regarded, described, referred, used, defined, seen, possible, served,\n",
      "Nearest to to: will, might, may, could, can, cannot, should, towards,\n",
      "Nearest to time: moment, period, decade, ingres, victory, night, times, championed,\n",
      "Nearest to th: nd, st, twentieth, nineteenth, rd, eicher, seventeenth, nine,\n",
      "Nearest to who: whom, which, alcaeus, trotskyists, he, already, young, observables,\n",
      "Nearest to it: he, she, this, there, farmed, mindedness, only, they,\n",
      "Nearest to not: never, now, still, nothing, almost, cypher, killings, captors,\n",
      "Nearest to years: decades, months, days, minutes, hours, year, weeks, centuries,\n",
      "Nearest to see: but, incorporates, includes, nitroglycerin, venue, saw, called, aoun,\n",
      "Nearest to used: referred, applied, described, seen, written, available, designed, found,\n",
      "Nearest to five: three, six, seven, four, eight, zero, nine, two,\n",
      "Nearest to one: carducci, burgh, greenpeace, carniola, hamath, two, madrid, overthrew,\n",
      "Nearest to may: can, might, should, will, could, would, must, cannot,\n",
      "Nearest to eight: nine, seven, six, five, three, four, zero, pastures,\n",
      "Nearest to been: become, occurred, evolved, remained, come, appeared, begun, increased,\n",
      "Average loss at step 82000: 2.929478\n",
      "Average loss at step 84000: 2.909056\n",
      "Average loss at step 86000: 2.922349\n",
      "Average loss at step 88000: 2.939968\n",
      "Average loss at step 90000: 2.833453\n",
      "Nearest to in: within, during, throughout, since, until, outside, on, of,\n",
      "Nearest to known: regarded, described, referred, defined, used, cited, seen, viewed,\n",
      "Nearest to to: will, must, might, cannot, may, can, should, would,\n",
      "Nearest to time: moment, period, distance, expense, point, outlets, mycoplasma, month,\n",
      "Nearest to th: st, nd, nineteenth, twentieth, fourth, rd, seventeenth, sixteenth,\n",
      "Nearest to who: whom, trotskyists, folds, he, which, forked, rahman, earnest,\n",
      "Nearest to it: he, she, this, farmed, there, mindedness, they, increasingly,\n",
      "Nearest to not: never, nothing, still, now, sometimes, emotive, also, cypher,\n",
      "Nearest to years: decades, months, days, minutes, hours, centuries, weeks, year,\n",
      "Nearest to see: includes, incorporates, allows, but, aoun, messagepad, saw, contains,\n",
      "Nearest to used: designed, referred, applied, required, available, intended, written, known,\n",
      "Nearest to five: four, six, seven, three, eight, zero, nine, two,\n",
      "Nearest to one: two, draw, four, abeda, zero, seven, ien, nine,\n",
      "Nearest to may: can, might, should, must, would, could, will, cannot,\n",
      "Nearest to eight: six, seven, four, nine, five, zero, three, pastures,\n",
      "Nearest to been: become, occurred, remained, interferon, evolved, gone, revolts, putin,\n",
      "Average loss at step 92000: 2.896135\n",
      "Average loss at step 94000: 2.887443\n",
      "Average loss at step 96000: 2.718379\n",
      "Average loss at step 98000: 2.458312\n",
      "Average loss at step 100000: 2.706570\n",
      "Nearest to in: within, during, throughout, outside, of, near, into, paleo,\n",
      "Nearest to known: regarded, described, used, cited, referred, defined, seen, viewed,\n",
      "Nearest to to: will, must, would, might, cannot, could, may, should,\n",
      "Nearest to time: point, moment, period, night, mycoplasma, decade, aimed, approximation,\n",
      "Nearest to th: nd, twentieth, nineteenth, st, fourth, seventeenth, rd, sixteenth,\n",
      "Nearest to who: whom, playfulness, meher, actually, typically, nuanced, jutta, trotskyists,\n",
      "Nearest to it: she, he, farmed, mindedness, onerous, this, there, jehoiachin,\n",
      "Nearest to not: never, nothing, now, still, almost, cypher, saloons, often,\n",
      "Nearest to years: decades, months, minutes, weeks, hours, days, centuries, year,\n",
      "Nearest to see: yer, references, incorporates, refer, contain, consider, but, includes,\n",
      "Nearest to used: referred, designed, intended, applied, known, cited, seen, employed,\n",
      "Nearest to five: six, eight, seven, four, three, zero, nine, two,\n",
      "Nearest to one: carducci, madrid, knudsen, bundy, staying, johannesburg, agassi, hairstyle,\n",
      "Nearest to may: might, must, should, can, would, could, will, cannot,\n",
      "Nearest to eight: seven, six, nine, five, four, three, zero, two,\n",
      "Nearest to been: become, occurred, remained, evolved, fallen, putin, gone, come,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(\n",
    "            batch_size, bag_window)\n",
    "        feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python TF",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
